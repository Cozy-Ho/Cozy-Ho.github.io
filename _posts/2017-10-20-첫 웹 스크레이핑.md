---
layout : post
title : 첫 웹 스크레이핑
date : 2017-10-20 09:15:23
tags:
- 웹크롤링
category: Web Scraping
---

**웹**이라는 것은 우리생각보다 간단하지 않다. 데이터 형식은 제 멋대로이고 닫는 태그도 빠져있는 경우가 많다. 그런 웹 환경에서 원하는 데이터를 긁어오려면 많은 상황과 변수를 예상할 수 있어야한다.

## Intro
<br>
웹 크롤링 포스트 시리즈는 <a href="https://zempirians.com/ebooks/Ryan%20Mitchell-Web%20Scraping%20with%20Python_%20Collecting%20Data%20from%20the%20Modern%20Web-O'Reilly%20Media%20(2015).pdf" target="_blank">O'REILLY - Web Scraping with Python</a>을 공부하며 정리하는 글이다.<br>
일단 우리가 매일 사용하는 `브라우저`가 얼마나 많은 일을 하고있는지 알아보자.<br>
영희는 웹 서버를 가지고있다. 철수는 데스크톰 컴퓨터를 가지고 영희의 서버에 연결하려 하고있다. 철수의 컴퓨터가 영희의 서버에 연결하려면 다음과같은 과정이 필요하다.

1. 철수의 컴퓨터는 1과0으로 된 비트 스트림을 보낸다. 각 비트는 전압으로 구별되고 정보를 구성하며, 헤더와 바디도 이런 정보의 조합으로 표현된다. 헤더에는 바로 다음목표인 철수의 라우터 MAC주소와 최종 목적지인 영희의 IP주소가 들어있다. 바디에는 철수가 영희의 서버 애플리케이션에 요청하는 내용이 들어있을 것 이다.
2. 철수의 라우터는 비트 스트림을 받아 철수의 MAC 주소에서 영희의 IP주소로 가는 패킷으로 해석한다. 철수의 라우터의 고유 IP주소를 패킷에 발신자 주소로 기록한 다음 이 패킷을 인터넷에 보낸다.
3. 철수의 패킷은 여러 중간 서버를 거치며 영희의 서버를 향해 패킷을 보낸다.
4. 영희의 서버는 그 패킷을 받는다.
5. 영희의 서버는 헤더에서 포트 번호를 찾고 적절한 애플리케이션에 보낸다.(보통 80포트=웹 애플리케이션)
6. 웹 서버 애플리케이션은 데이터 스트림을 받는다. 이 데이터에는 다음과같은 정보가 들어있다.
   - 이 요청은 GET요청이다.
   - 요청하는 파일은 index.html 이다.
7. 웹 서버는 해당하는 HTML 파일을 찾아 새 패킷으로 묶은 뒤 영희의 라우터를 통해 철수의 컴퓨터로 전송한다. 이 패킷은 철수가 보낸 패킷과 비슷한 과정을 거쳐 철수의 컴퓨터에 도달한다.
*<center>전체과정이 1초도 안되서 이루어진다</center>*

<br>
여기서 `브라우저`는 패킷을 만들고, 보내고, 돌아온 데이터를 해석해 사진, 소리, 비디오 텍스트 등으로 표현하는 프로그램이다. 이 `브라우저`는 프로세서에 명령을 내려 데이터를 애플리케이션에 보내서 유/무선 인터페이스로 처리할 수 있고, 그런 일을 하는 라이브러리가 여러 언어에 존재한다.

물론 파이썬에서도 가능하다.
```python
from urllilb.request import urlopen
html = urlopen("https://google.com/")
print(html.read())
```

이 코드를 `test.py`로 저장하고 실행시켜보자.

출력결과는 `www.google.com` 페이지의 HTML 코드 전체이다. `urllib`는 파이썬 표준 라이브러리이므로 따로 설치할 것은 없다.

---

### BeautifulSoup
~~아름다운 수프~~

이름 그대로 이 라이브러리는 잘못된 HTML을 수정하여 아름답게 만드는 역할을한다. 기본 파이썬 라이브러리가 아니므로 반드시 설치를 해야한다.
> $ pip install beautifulsoup4

BeautifulSoup4(BS4)를 본격적으로 활용해보자.

```python
from urllib.request import urlopen
from bs4 import BeautifulSoup

html = urlopen("https://en.wikipedia.org/wiki/Web_scraping")
bsObj = BeautifulSoup(html.read(), "html.parser")

print(bsObj.h1)
```
출력 결과는 다음과 같다.

> \<h1>Web scraping\</h1>

<h2><a href="https://cozy-ho.github.io/web%20scraping/2017/10/23/HTML%EB%B6%84%EC%84%9D.html" target="_blank">다음포스트 - BS4 활용하기</a></h2>
